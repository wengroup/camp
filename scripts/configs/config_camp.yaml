seed_everything: 35
default_dtype: float32
restore_checkpoint: null # path to checkpoint to restore from; if `null`, train from scratch

data:
  atomic_number: [3, 15, 16]
  trainset_filename: /Users/mjwen.admin/Packages/camp_analysis/dataset/nequip_LiPS/json_data/train_10_LiPS.json
  valset_filename: /Users/mjwen.admin/Packages/camp_analysis/dataset/nequip_LiPS/json_data/train_100_LiPS.json
  testset_filename: /Users/mjwen.admin/Packages/camp_analysis/dataset/nequip_LiPS/json_data/train_100_LiPS.json
  r_cut: 5.0
  train_batch_size: 4
  val_batch_size: 20
  test_batch_size: 20

model:
  num_layers: 2 # number of layers
  num_average_neigh: auto # average number of neighbors (float), if `auto` determined automatically from training set

  # angular
  max_u: 4 # max radial degree
  max_v: 2 # max angular degree

  # radial
  radial_mlp_hidden_layers: [32, 32]
  max_chebyshev_degree: 8 # max degree of Chebyshev polynomial for the radial part

  # output
  output_mlp_hidden_layers: [32, 32]

loss:
  energy_ratio: 1.0
  forces_ratio: 1.0
  normalize: true # normalize energy by number of atoms

metrics:
  type: mae
  normalize: true # normalize energy by number of atoms
  validation_start_epoch: 0 # start calculating metrics at this epoch

optimizer:
  class_path: torch.optim.Adam
  init_args:
    amsgrad: true
    lr: 0.01
    weight_decay: 0.0

lr_scheduler:
  monitor: val/mae_f
  class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
  init_args:
    mode: min
    factor: 0.8
    patience: 100
    verbose: true
#lr_scheduler:
#  class_path: torch.optim.lr_scheduler.CosineAnnealingLR
#  init_args:
#    T_max: 5 # should be equal to trainer.max_epochs
#    eta_min: 0.0001

ema: # exponential moving average, https://github.com/lucidrains/ema-pytorch
  beta: 0.999
  update_after_step: 1000
  update_every: 10
  power: 0.6667
  include_online_model: false

trainer:
  max_epochs: 5
  accelerator: cpu
  num_nodes: 1
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  detect_anomaly: false
  inference_mode: false # enable calculation of gradients in val/test step, must be false for inference
  gradient_clip_val: 100.0

  logger:
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      project: camp_proj

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: -1
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: null
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_ema/mae_e
        mode: min
        save_top_k: 3
        verbose: false
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_ema/mae_e
        mode: min
        patience: 200
        verbose: true
#    - class_path: lightning.pytorch.callbacks.StochasticWeightAveraging
#      init_args:
#        swa_lrs: 0.001
#        swa_epoch_start: 0.8
